{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqDFk8ut7Ww/Ymh06D1SE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ash-rulz/TextMining/blob/main/EvalDataGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code to generate the gold standard question-answer pairs for the RAGAS evaluation framework.\n",
        "\n",
        "The gold standard data is pulled from a [website](https://hobbylark.com/party-games/180-Printable-Trivia-Questions-for-Harry-Potter-and-the-Sorcerers-Stone) containing trivia of the book Harry Potter and the Sorcer's stone. In addition to the data massaging done below, some of the text was manually corrected and even some of the ambiguous question-answer pairs were removed."
      ],
      "metadata": {
        "id": "1wmB27bJn555"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2"
      ],
      "metadata": {
        "id": "v4qlPVHXLj2e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pqIPN9H62kVO"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "import csv\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Place QAEval.pdf in the folder and run the below code to generate QAEval.csv"
      ],
      "metadata": {
        "id": "YJ3BFQLMmIIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate questions and answers list\n",
        "pdf_path = 'QAEval.pdf'\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "with open(pdf_path, 'rb') as pdf_file:\n",
        "  pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "  num_pages = len(pdf_reader.pages)\n",
        "\n",
        "  for page_num in range(num_pages):\n",
        "    page = pdf_reader.pages[page_num]\n",
        "    text = page.extract_text()\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    iter_lines = iter(lines)\n",
        "\n",
        "    for line in iter_lines:\n",
        "      if line.strip().endswith('?'):\n",
        "        line = line.replace('https://hobbylark.com/party-games/180-Printable-Trivia-Questions-for-Harry-Potter-and-the-Sorcerers-Stone ', '')\n",
        "        line = line.replace('’', \"'\")\n",
        "        questions.append(re.sub(r'^\\d+[\\/\\d+]*\\.\\s*', '', line.strip()))\n",
        "        answer = next(iter_lines).strip().replace('’', \"'\").replace('Top 30 Banish E ffects in \"Y u-Gi-Oh!\"','')\n",
        "        answers.append(answer)\n"
      ],
      "metadata": {
        "id": "4kk1vWqj21N_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save questions and answers to CSV\n",
        "csv_path = 'QAEval.csv'\n",
        "with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Question', 'Answer']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write questions and answers\n",
        "    for question, answer in zip(questions, answers):\n",
        "        writer.writerow({'Question': question, 'Answer': answer})"
      ],
      "metadata": {
        "id": "vOZuwTdDM3FL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few additional changes(manual) were made like there were unnecessary empty spaces which was removed. Few of the question-answer pairs were also removed, as they seemed a bit ambiguous. The final evaluation dataset can be found [here](https://github.com/ash-rulz/TextMining/blob/main/QAEval.csv)."
      ],
      "metadata": {
        "id": "McjTInXBmjLv"
      }
    }
  ]
}